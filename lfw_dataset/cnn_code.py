# -*- coding: utf-8 -*-
"""cnn_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1llINKsT6l_fB3G9i3x0SGnmoCgwdO8eS
"""

!unzip /content/custom_dataset.zip

from google.colab import drive
drive.mount('/content/drive')

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import os
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D , Flatten, Activation, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import models, layers, optimizers, regularizers, utils
from tensorflow.keras import applications
from tensorflow.keras.losses import categorical_crossentropy,categorical_hinge,hinge,squared_hinge
from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization , Input ,concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import  EarlyStopping, ModelCheckpoint
from tensorflow.keras.preprocessing.image import ImageDataGenerator

import numpy as np
import pandas as pd 
from sklearn.model_selection import train_test_split

import cv2

import matplotlib.pyplot as plt
from tensorflow.keras.applications.vgg16 import VGG16
from datetime import datetime
import io
import itertools
from packaging import version



from tensorflow.keras import backend as K
import matplotlib.pyplot as plt
import numpy as np

import sklearn.metrics
from tensorflow.keras.optimizers import SGD

# %load_ext tensorboard

def normalize_mean_var(arr):
    arr -= np.mean(arr)
    arr /= np.var(arr)
    return arr

number_of_images = 20
pth = "/content/lfw_allnames.csv" 
iter_csv = pd.read_csv(pth, iterator=True, chunksize=1000)
df = pd.concat([chunk[chunk['images'] > number_of_images] for chunk in iter_csv])
topNames = set([x for x in df['name']])
len(topNames)

import math
class my_gen_extra(tf.keras.utils.Sequence):

    def __init__(self, data_path):
        
        self.data_arr = np.load(data_path, allow_pickle=True)
      
    def __len__(self):
        return len(self.data_arr)

    def __getitem__(self, idx):
        return self.data_arr[idx][0], self.data_arr[idx][1]
    
    def get_data(self):
        return self.data_arr
    
train_path = "/content/Train_imc_50.npy"
valid_path = "/content/valid_imc_50.npy"

train_generator = my_gen_extra(train_path)
validation_generator = my_gen_extra(valid_path)

from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization , Input ,concatenate
def CNN_Model(num_classes):
    
    vgg16 = applications.VGG16(include_top=False, input_shape=(224, 224, 3), weights='imagenet')
    for layer in vgg16.layers[:-6]:
        layer.trainable = False

    x = BatchNormalization()(vgg16.output)
    x = layers.Flatten()(x)
    x = layers.Dropout(0.5)(x)
    
    x = layers.Dense(2048,name = 'Dense1_MERGE', kernel_regularizer=regularizers.l2(0.001))(x)
    x = layers.PReLU(alpha_initializer='zero', weights=None, name = 'PReLU1_MERGE')(x)
  
    
    x = layers.Dropout(0.5, name = 'Dropout2_MERGE')(x)
    x = Dense(num_classes, activation='softmax', name = 'Dens2e_MERGE')(x)
    

    model = models.Model(inputs= vgg16.input, outputs= x)
    
    optimizer = tf.keras.optimizers.Adam(
        learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=True,
        name='Adam')

    model.compile(loss=categorical_crossentropy, optimizer=optimizer, metrics=['accuracy'])
    return model

model = CNN_Model(12)

model.summary()

history = model.fit_generator(
    generator=train_generator,
    validation_data=validation_generator,
    steps_per_epoch=len(train_generator),
    epochs=30
)

import matplotlib.pyplot as plt
def show_plot(x_data, y_data, x_label, y_label, title):
    plt.subplots(figsize=(12*2, 12))
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.plot(x_data, y_data)
    plt.plot(x_data, y_data)
    plt.title(title)
    plt.show()
    
acc = np.asarray(history.history['val_accuracy'][:53])

show_plot([i for i in range(1,len(acc)+1)], acc, 'Epochs', 'Accuracy', 'CNN VGG16 Pretrained Accuracy')

from keras import backend as K

def feature_model_func(model):
    inp = model.input                                         
    outputs = model.layers[-2].output
    functor = K.function(inp, outputs)   
    return functor

feature_model = feature_model_func(model)

len(feature_model([train_generator[0][0]])[0])

train_generator[0][0].shape

def label_converter(arr):
    for i in range(len(arr)):
        if arr[i] == 1:
            return i
    return -1

def remove_batches(gen):
    X = []
    y = []
    for i in range(len(gen)):
        im_pred_batch = feature_model(gen[i][0])
        lbl_batch = gen[i][1]
        for j in range(len(im_pred_batch)):
            X.append(im_pred_batch[j])
            y.append(label_converter(lbl_batch[j]))
        
    return np.asarray(X), np.asarray(y)

X_training, y_training = remove_batches(train_generator)

X_validation, y_validation = remove_batches(validation_generator)

cnn_train_path = "/content/cnn_train_imc_50.npy"
cnn_valid_path = "/content/cnn_valid_imc_50.npy"

np.save(cnn_train_path, X_training)
np.save(cnn_valid_path, X_validation)

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import accuracy_score

def confusion_matrix_display(model, X_test, y_test, title):
    y_pred = model.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    labels = np.unique(y_test)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
    disp.plot(cmap=plt.cm.Blues)
    disp.ax_.set_title('Confusion matrix: ' + title)

from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics

clf = RandomForestClassifier(n_estimators=100)
clf.fit(X_training, y_training)

pred = clf.predict(X_validation)
cnf_matrix = metrics.confusion_matrix(y_validation, pred)
RF_acc = metrics.accuracy_score(y_validation, pred)

print("Random Forest Accuracy:", RF_acc)
confusion_matrix_display(clf, X_validation, y_validation, "Random Forest")

from sklearn.svm import LinearSVC
def Linear_SVM_classification(X_train, X_test, y_train, y_test):
    svc= LinearSVC(C=100.0, random_state=42, max_iter=1000, verbose=1)
    svc.fit(X_train, y_train)

    svcpred = svc.predict(X_test)
    cnf_matrix = metrics.confusion_matrix(y_test, svcpred)
    SVC_acc = metrics.accuracy_score(y_test, svcpred)
    
    print('Linear SVC accuracy: {}'.format(SVC_acc))
    confusion_matrix_display(svc, X_test, y_test, "Linear SVM")
        
Linear_SVM_classification(X_training, X_validation, y_training, y_validation)

from sklearn.svm import SVC

def SVM_classification(X_train, X_test, y_train, y_test):
    svc = SVC(kernel='rbf', gamma='scale')
    svc.fit(X_train, y_train)
    svcpred = svc.predict(X_test)
    cnf_matrix = metrics.confusion_matrix(y_test, svcpred)
    SVC_acc = metrics.accuracy_score(y_test, svcpred)

    print('SVM with RBF kernel accuracy: {}'.format(SVC_acc))
    confusion_matrix_display(svc, X_test, y_test, "SVM with RBF kernel")

SVM_classification(X_training, X_validation, y_training, y_validation)

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score

def gradient_boosting_classification(X_train, X_test, y_train, y_test):
    model = GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print("Gradient Boosting accuracy:", accuracy)
    confusion_matrix_display(model, X_test, y_test, "Gradient Boosting")

gradient_boosting_classification(X_training, X_validation, y_training, y_validation)