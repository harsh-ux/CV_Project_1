# -*- coding: utf-8 -*-
"""EigenFaces_Feature_Extraction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-IH1YHJpcp8ewZQLR3stPCvoRxlW6rPg

EigenFaces Analysis
"""

!7z x /content/miniCelebA.tar.gz
# !unzip /content/custom.zip

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.datasets import fetch_lfw_people

lfw_dataset = fetch_lfw_people(min_faces_per_person=70, resize=0.4)

X = lfw_dataset.data.astype('float32')
X -= np.mean(X, axis=0)
X /= np.std(X, axis=0)

pca = PCA(n_components=150, whiten=True)
X_pca = pca.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_pca, lfw_dataset.target, test_size=0.2, random_state=42)

param_grid = {
    'learning_rate': [0.1, 0.05, 0.02, 0.01],
    'max_depth': [4, 6],
    'min_samples_leaf': [3, 5, 9, 17],
    'max_features': [1.0, 0.3, 0.1]
}

gbc = GradientBoostingClassifier(n_estimators=100)
grid_search = GridSearchCV(gbc, param_grid=param_grid, cv=5, n_jobs=-1)
grid_search.fit(X_train, y_train)
gbc_best = grid_search.best_estimator_

y_pred = gbc_best.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')

fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(16, 4))
for i, idx in enumerate(np.random.choice(X_test.shape[0], size=4, replace=False)):
    ax[i].imshow(pca.inverse_transform(X_test[idx]).reshape((50, 37)), cmap='gray')
    ax[i].set_title('True: {}\nPredicted: {}'.format(lfw_dataset.target_names[y_test[idx]], lfw_dataset.target_names[y_pred[idx]]))
    ax[i].axis('off')
plt.show()


fig, ax = plt.subplots()
ax.bar(np.arange(len(gbc_best.feature_importances_)), gbc_best.feature_importances_)
ax.set_xlabel('Feature Index')
ax.set_ylabel('Importance')
ax.set_title('Feature Importance for Gradient Boosting Classifier with PCA Features')
plt.show()

print('Gradient Boosting Classifier with PCA Features')
print('Accuracy:', accuracy)
print('Precision:', precision)
print('Recall:', recall)

"""SVM with PCA Features"""

from sklearn.svm import SVC

svm = SVC(kernel='rbf', class_weight='balanced')
param_grid = {
    'C': [1e-3, 1e-2, 1e-1, 1, 10, 100],
    'gamma': [0.001, 0.01, 0.1, 1]
}
grid_search = GridSearchCV(svm, param_grid=param_grid, cv=5, n_jobs=-1)
grid_search.fit(X_train, y_train)
svm_best = grid_search.best_estimator_

y_pred = svm_best.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')

fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(16, 4))
for i, idx in enumerate(np.random.choice(X_test.shape[0], size=4, replace=False)):
    ax[i].imshow(pca.inverse_transform(X_test[idx]).reshape((50, 37)), cmap='gray')
    ax[i].set_title('True: {}\nPredicted: {}'.format(lfw_dataset.target_names[y_test[idx]], lfw_dataset.target_names[y_pred[idx]]))
    ax[i].axis('off')
plt.show()

print('Support Vector Machine with PCA Features')
print('Accuracy:', accuracy)
print('Precision:', precision)
print('Recall:', recall)

print(svm_best)

from sklearn.feature_selection import SelectFromModel
from sklearn.svm import LinearSVC

lsvc = LinearSVC(C=10, class_weight='balanced', dual=False)
lsvc.fit(X_train, y_train)

model = SelectFromModel(lsvc, prefit=True)
X_new = model.transform(X_train)

svc_new = SVC(C=10, class_weight='balanced', gamma=0.001)
svc_new.fit(X_new, y_train)

X_test_new = model.transform(X_test)

y_pred = svc_new.predict(X_test_new)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')

print('SVC Classifier with LinearSVC Features')
print('Accuracy:', accuracy)
print('Precision:', precision)
print('Recall:', recall)

idxs = model.get_support(indices=True)

fig, ax = plt.subplots()
ax.bar(np.arange(len(idxs)), lsvc.coef_.reshape(-1)[idxs])
ax.set_xlabel('Feature Index')
ax.set_ylabel('Coefficient')
ax.set_title('Feature Importance for LinearSVC Classifier with SVC Features')
plt.show()

"""Random Forest with PCA Features"""

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(n_estimators=100, class_weight='balanced')
param_grid = {
    'max_depth': [4, 8, 16, 32, None],
    'max_features': ['sqrt', 'log2', 0.2, 0.5, 0.8],
    'min_samples_split': [2, 4, 8],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}
grid_search = GridSearchCV(rfc, param_grid=param_grid, cv=5, n_jobs=-1)
grid_search.fit(X_train, y_train)
rfc_best = grid_search.best_estimator_

y_pred = rfc_best.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')

fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(16, 4))
for i, idx in enumerate(np.random.choice(X_test.shape[0], size=4, replace=False)):
    ax[i].imshow(pca.inverse_transform(X_test[idx]).reshape((50, 37)), cmap='gray')
    ax[i].set_title('True: {}\nPredicted: {}'.format(lfw_dataset.target_names[y_test[idx]], lfw_dataset.target_names[y_pred[idx]]))
    ax[i].axis('off')
plt.show()

fig, ax = plt.subplots()
ax.bar(np.arange(len(rfc_best.feature_importances_)), rfc_best.feature_importances_)
ax.set_xlabel('Feature Index')
ax.set_ylabel('Importance')
ax.set_title('Feature Importance for Random Forest Classifier with PCA Features')
plt.show()

print('Random Forest Classifier with PCA Features')
print('Accuracy:', accuracy)
print('Precision:', precision)
print('Recall:', recall)

!7z x /content/miniCelebA.tar.gz

import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.model_selection import train_test_split, GridSearchCV

data_dir = "minicelebA"
X = np.load(os.path.join(data_dir, "faces.npy"))
y = np.load(os.path.join(data_dir, "labels.npy"))

X = X.astype('float32')
X -= np.mean(X, axis=0)
X /= np.std(X, axis=0)

pca = PCA(n_components=150, whiten=True)
X_pca = pca.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)

rfc = RandomForestClassifier(n_estimators=100, class_weight='balanced')
param_grid = {
    'max_depth': [4, 8, 16, 32, None],
    'max_features': ['sqrt', 'log2', 0.2, 0.5, 0.8],
    'min_samples_split': [2, 4, 8],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}
grid_search = GridSearchCV(rfc, param_grid=param_grid, cv=5, n_jobs=-1)
grid_search.fit(X_train, y_train)
rfc_best = grid_search.best_estimator_

y_pred = rfc_best.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')

fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(16, 4))
for i, idx in enumerate(np.random.choice(X_test.shape[0], size=4, replace=False)):
    ax[i].imshow(pca.inverse_transform(X_test[idx]).reshape((64, 64)), cmap='gray')
    ax[i].set_title('True: {}\nPredicted: {}'.format(y_test[idx], y_pred[idx]))
    ax[i].axis('off')
plt.show()

fig, ax = plt.subplots()
ax.bar(np.arange(len(rfc_best.feature_importances_)), rfc_best.feature_importances_)
ax.set_xlabel('Feature Index')
ax.set_ylabel('Importance')
ax.set_title('Feature Importance for Random Forest Classifier with PCA Features')
plt.show()

# Print the evaluation metrics
print('Random Forest Classifier with PCA Features')
print('Accuracy:', accuracy)
print('Precision:', precision)
print('Recall:', recall)

import numpy as np
from keras_vggface.vggface import VGGFace
from keras_vggface import utils
from scipy import misc
from keras import Model
from keras.utils import to_categorical
from keras.layers import GlobalAveragePooling2D
import os
from glob import glob


def get_deep_feature(x):

    model = VGGFace(include_top=False, input_shape=(224, 224, 3),
                    pooling='avg')  # pooling: None, avg or max
    output = model.get_layer('conv5_3').output
    output = GlobalAveragePooling2D()(output)
    feature_model = Model(inputs=model.input, outputs=output)

    x = utils.preprocess_input(x, version=1)  # or version=2
    x = feature_model.predict(x)

    return x


def prepare_data(src, dst):

    data_prefix = 'miniCelebA_'
    for split in ['train', 'val', 'test']:
        print('processing %s split' % split)
        if (not os.path.exists(os.path.join(dst, 'x_' + split + '.npy')) or not
                os.path.exists(os.path.join(dst, 'y_' + split + '.npy'))):
            labels = glob(os.path.join(src, split, '*'))
            no_sample = 0
            for lb in labels:
                no_sample += len(os.listdir(lb))

            x = np.zeros((no_sample, 224, 224, 3))
            y = np.zeros((no_sample, 20))
            count = 0
            for lb in labels:
                files = glob(os.path.join(lb, '*.png'))
                for f in files:
                    print('processing file: %s, with label %s' % (f, lb.split('/')[-1]))
                    y[count] = to_categorical(int(lb.split('/')[-1]), 20)
                    img = misc.imresize(misc.imread(f), (224, 224), 'bicubic')
                    if img.ndim == 2:
                        img = np.expand_dims(img, -1)
                        img = np.concatenate((img, img, img), axis=-1)
                    x[count] = img

                    count += 1

            assert count == no_sample, "number of sample (%d) is different than number of read image (%d)" % (
                no_sample, count)

            x = get_deep_feature(x)
            np.save(os.path.join(dst, data_prefix + 'x_' + split + '.npy'), x)
            np.save(os.path.join(dst, data_prefix + 'y_' + split + '.npy'), y)


src = '/content/miniCelebA'
dst = '/content/datan'

prepare_data(src, dst)

!pip install keras_applications