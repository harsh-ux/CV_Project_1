# -*- coding: utf-8 -*-
"""eigenfaces_custom.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SO07KcYBYNC6WqhS3ApFKjUZwqDBmouv

#Custom Dataset
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.model_selection import train_test_split, GridSearchCV
import os
import cv2
data_path = '/content/custom'

n_images_per_subfolder = 2

X = []
y = []
for i, folder_name in enumerate(os.listdir(data_path)):
    folder_path = os.path.join(data_path, folder_name)
    if os.path.isdir(folder_path):
        image_files = os.listdir(folder_path)
        if len(image_files) == n_images_per_subfolder:
            for image_file in image_files:
                image_path = os.path.join(folder_path, image_file)
                image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
                X.append(image)
                y.append(i)

X = np.array(X)
y = np.array(y)

X = X.astype('float32')
X -= np.mean(X, axis=0)
X /= np.std(X, axis=0)

pca = PCA(n_components=150, whiten=True)
X_pca = pca.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)

param_grid = {
    'learning_rate': [0.1, 0.05, 0.02, 0.01],
    'max_depth': [4, 6],
    'min_samples_leaf': [3, 5, 9, 17],
    'max_features': [1.0, 0.3, 0.1]
}

gbc = GradientBoostingClassifier(n_estimators=100)
grid_search = GridSearchCV(gbc, param_grid=param_grid, cv=5, n_jobs=-1)
grid_search.fit(X_train, y_train)
gbc_best = grid_search.best_estimator_

y_pred = gbc_best.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')

fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(16, 4))
for i, idx in enumerate(np.random.choice(X_test.shape[0], size=4, replace=False)):
    ax[i].imshow(pca.inverse_transform(X_test[idx]).reshape((50, 37)), cmap='gray')
    ax[i].set_title('True: {}\nPredicted: {}'.format(lfw_dataset.target_names[y_test[idx]], lfw_dataset.target_names[y_pred[idx]]))
    ax[i].axis('off')
plt.show()

fig, ax = plt.subplots()
ax.bar(np.arange(len(gbc_best.feature_importances_)), gbc_best.feature_importances_)
ax.set_xlabel('Feature Index')
ax.set_ylabel('Importance')
ax.set_title('Feature Importance for Gradient Boosting Classifier with PCA Features')
plt.show()

print('Gradient Boosting Classifier with PCA Features')
print('Accuracy:', accuracy)
print('Precision:', precision)
print('Recall:', recall)

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(n_estimators=100, class_weight='balanced')
param_grid = {
    'max_depth': [4, 8, 16, 32, None],
    'max_features': ['sqrt', 'log2', 0.2, 0.5, 0.8],
    'min_samples_split': [2, 4, 8],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}
grid_search = GridSearchCV(rfc, param_grid=param_grid, cv=5, n_jobs=-1)
grid_search.fit(X_train, y_train)
rfc_best = grid_search.best_estimator_

y_pred = rfc_best.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')

fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(16, 4))
for i, idx in enumerate(np.random.choice(X_test.shape[0], size=4, replace=False)):
    ax[i].imshow(pca.inverse_transform(X_test[idx]).reshape((50, 37)), cmap='gray')
    ax[i].set_title('True: {}\nPredicted: {}'.format(lfw_dataset.target_names[y_test[idx]], lfw_dataset.target_names[y_pred[idx]]))
    ax[i].axis('off')
plt.show()

fig, ax = plt.subplots()
ax.bar(np.arange(len(rfc_best.feature_importances_)), rfc_best.feature_importances_)
ax.set_xlabel('Feature Index')
ax.set_ylabel('Importance')
ax.set_title('Feature Importance for Random Forest Classifier with PCA Features')
plt.show()

print('Random Forest Classifier with PCA Features')
print('Accuracy:', accuracy)
print('Precision:', precision)
print('Recall:', recall)

from sklearn.feature_selection import SelectFromModel
from sklearn.svm import LinearSVC

lsvc = LinearSVC(C=10, class_weight='balanced', dual=False)
lsvc.fit(X_train, y_train)

model = SelectFromModel(lsvc, prefit=True)
X_new = model.transform(X_train)

svc_new = SVC(C=10, class_weight='balanced', gamma=0.001)
svc_new.fit(X_new, y_train)

X_test_new = model.transform(X_test)

y_pred = svc_new.predict(X_test_new)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')

print('SVC Classifier with LinearSVC Features')
print('Accuracy:', accuracy)
print('Precision:', precision)
print('Recall:', recall)

idxs = model.get_support(indices=True)

fig, ax = plt.subplots()
ax.bar(np.arange(len(idxs)), lsvc.coef_.reshape(-1)[idxs])
ax.set_xlabel('Feature Index')
ax.set_ylabel('Coefficient')
ax.set_title('Feature Importance for LinearSVC Classifier with SVC Features')
plt.show()

from sklearn.metrics import plot_confusion_matrix

plot_confusion_matrix(rfc_best, X_test, y_test, display_labels=lfw_dataset.target_names, cmap=plt.cm.Blues)
plt.title('Confusion Matrix for Random Forest Classifier')
plt.show()

plot_confusion_matrix(svc_new, X_test_new, y_test, display_labels=lfw_dataset.target_names, cmap=plt.cm.Blues)
plt.title('Confusion Matrix for Support Vector Classifier with LinearSVC Features')
plt.show()

from sklearn.metrics import plot_roc_curve

plot_roc_curve(rfc_best, X_test, y_test)
plt.title('ROC Curve for Random Forest Classifier')
plt.show()

plot_roc_curve(svc_new, X_test_new, y_test)
plt.title('ROC Curve for Support Vector Classifier with LinearSVC Features')
plt.show()

from sklearn.metrics import plot_precision_recall_curve

plot_precision_recall_curve(rfc_best, X_test, y_test)
plt.title('Precision-Recall Curve for Random Forest Classifier')
plt.show()

plot_precision_recall_curve(svc_new, X_test_new, y_test)
plt.title('Precision-Recall Curve for Support Vector Classifier with LinearSVC Features')
plt.show()